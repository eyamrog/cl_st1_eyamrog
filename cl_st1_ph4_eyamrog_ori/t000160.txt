Purpose: This study analyzed the content validity and reliability of the Questionnaire for 
Screen Time of Adolescents (QueST). Methods: QueST measures screen time across five 
constructs: studying, working/internship-related activities, watching videos, playing video 
games, and using social media/chat applications. The content validity, including a pretest, was 
carried out by experts and adolescents. For reliability analysis, QueST was applied and 
reapplied after one week in a sample of 104 adolescents (16.3 ± 1.02 years; 66.3% girls). 
Results: The Content Validity Index for Scales indicated 94% and 98% of overall clarity and 
representativeness, respectively. The QueST was considered comprehensible and clear by 
adolescents. The intraclass correlation coefficients ranged from 0.41 (95% CI 0.24, 0.56) for 
videos to 0.76 (95% CI 0.66, 0.83) for social media/chat applications on a weekday, and from 
0.24 (95% CI 0.04; 0.41) for videos to 0.67 (95% CI 0.54; 0.77) for social media/chat 
applications on weekends. Conclusions: The QueST has demonstrated satisfactory content 
validity; however, measuring the time watching videos during free-living is a challenge for 
researchers. In general, the QueST is recommended to measure different screen time constructs. 

Screen time behaviors is a term to describe behaviors that imply interaction with 
electronic devices (e.g., watching television; using smartphones) and may be performed 
recreationally, professionally, and in educational settings (Tremblay et al., 2017). Screen time, 
frequently in the form of television viewing, computer using, or video game playing, has been 
related to unhealthy outcomes among children and adolescents (Biddle et al., 2017; Biswas et 
al., 2015; Carson et al., 2016; de Rezende et al., 2014; Tremblay et al., 2011). Behaviors like 
watching movies and videos were usually limited to television devices, and playing videogames 
required specific consoles until recently; however, with the advancement of technology, these 
activities are viewable on several gadgets, including computers, tablets, and smartphones. 
These innovations caused changes in screen time behaviors such as the decrease in television 
use and increased computer use among adolescents (Bucksch et al., 2016; Silva et al., 2014). 
As the diversity of activities done on electronic screens is continually evolving, the impact of 
these activities on health also changes. For example, the World Health Organization 
incorporated video game addiction into the International Classification of Diseases-11, 
describing that addiction to electronic games negatively affects the individuals’ health (World 
Health Organization, 2018). Another example is the excessive social media usage, which is 
relatively novel, and has been associated with depressive symptoms (da Costa et al., 2020), 
socialization problems (Arundell et al., 2019; Devine & Lloyd, 2012; Ihm, 2018), poor body 
image (de Vries et al., 2016), and poor academic performance (Kuss & Griffiths, 2011). 

The diversity of activities that can be performed on each electronic device (e.g., it is 
possible to play, watch videos, and access social media on computers and smartphones) brings 
new challenges to the measurement of screen time. Two reviews of questionnaires for 
measuring sedentary behavior demonstrated that most instruments include a single question, 
often measuring the time watching television, playing video games, and/or using computers 
(Hidding et al., 2017; Prince et al., 2017). However, as the relationship of each of these activities 
with health outcomes may differ (Biddle et al., 2017; Carson et al., 2016; da Costa et al., 2020; 
Ihm, 2018; Weaver et al., 2010), it is still imperative to identify the different activities in order 
to broaden the understanding of the etiology of health problems in pediatric populations. Thus, 
this study aims to propose a questionnaire to measure different constructs of screen time among 
adolescents and evaluate its content validity and reliability. 

The Questionnaire for Screen Time of Adolescents (QueST) was designed for 
assessing habitual volumes of screen time in different constructs for the adolescent population. 
After the initial development of the QueST, it went through three steps of psychometric 
evaluation, with each step being conducted with a different sample, as follows: i) for the content 
validity, 16 experts in the research field of screen time among adolescents were included; ii) 
for pretesting the questionnaire, 14 adolescents from a Federal Institute of Technological 
Education of Santa Catarina state were recruited; iii) lastly, for reliability, a sample of 104 high 
school students from the Aplicação school, Santa Catarina state, was analyzed. These three 
steps were conducted in 2019. All adolescents and their parents/legal guardians approved the 
study protocols and provided written consent forms. This study was approved by the ethics 
committee for research with human participants of the Federal University of Santa Catarina, 
Brazil (protocol number: 3.168.745).   

The QueST aims to measure screen time during weekdays and weekends across 
different constructs. The initial construction of the instrument followed standardized 
recommendations (Hidding et al., 2017) and begun after a non-systematic consultation of recent 
reviews of questionnaires for measuring sedentary behavior (Hidding et al., 2017; Prince et al., 
2017). The development procedures of the QueST can be described as follows: i) identification 
of the constructs; ii) determination of the administration format of the questionnaire; iii) choice 
of the number, format, order, and text of the items and response options; iv) review of the 
questionnaire and optimization of its organization and readability (de Vet et al., 2011; Tsang et 
al., 2017). 

Five screen time constructs were defined based on questions used in research related 
to sedentary behavior (Cerin et al., 2014; Guimarães et al., 2013; Hidding et al., 2017; Prince 
et al., 2017; Treuth et al., 2003), as follows: (i) activities related to study or homework; (ii) 
activities related to work (including internships and non-profit activities); (iii) watching videos, 
such as series, movies, news, and sports; (iv) playing video games; and (v) use of social media 
and chat applications. The choice to measure the use of chat applications and social media 
within a single construct was made as platforms and applications generally offer both services 
(e.g., it is possible to send direct messages to other users on Facebook, Instagram, and Twitter). 
The work-related construct was included as some internships and jobs require screen time 
activities. For each construct, the time in hours and minutes can be reported during weekdays 
and weekend days. 

The QueST was initially written in Brazilian Portuguese and designed to be self-
administered by adolescents using a smartphone, tablet, or computer with access to the internet. 
The instrument was hosted at SurveyMonkey® platform. Each of the described constructs 
represented an item in the questionnaire. All items were described with the following 
instructions: “Insert zero if you do not engage in these kinds of activity” and followed by an 
answering example (e.g., “Example: I watch series for 1 and a half hours per day [insert 1 in 
the field of hours and 30 in the field of minutes]”). QueST items are shown in Table 1. 

For the content validation, a team of experts was selected among those who had 
ongoing research projects, monographic productions, and articles published in scientific 
journals about screen time behaviors or studies with psychometrics and validation of 
questionnaires. All experts had a doctoral title and were either professors or researchers in 
universities or research institutes. The experts were contacted by e-mail, where they received 
an invitation letter introducing the QueST and explaining the rationale for its development, it 
included a background text including key concepts and an explanation for its application and 
use in research. 

The experts evaluated the QueST in two steps: (i) an individual evaluation of each of 
the items, and (ii) a global evaluation of the QueST (Polit & Beck, 2006). The experts rated the 
content validity of the questionnaire independently, evaluating each item regarding clarity and 
representativeness (Rubio et al., 2003). The clarity evaluation aimed to rate the writing of the 
questions considering the comprehension of the construct being measured (Grant & Davis, 
1997). Whereas, the representativeness evaluation aimed to verify if the items reflected screen 
time, its constructs and concepts (Grant & Davis, 1997). The experts analyzed each item and 
the response scale, then they answered about clarity through a 4-point Likert scale (4 = highly 
clear; 3 = quite clear; 2 = somewhat clear; 1 = not clear), as well as, they answered about the 
representativeness of the constructs being measured using a similar scale (4 = the item is 
representative; 3 = the item needs minor revisions to be representative; 2 = the item needs major 
revisions to be representative; 1 = the item is not representative) (Rubio et al., 2003). When 
considering the ratings on clarity and representativeness, the Content Validity Index for each 
question was computed (Polit & Beck, 2006). Besides, general comments on the questions 
could be added by the experts.  

For the global evaluation of the QueST, experts answered about the clarity and 
expressiveness of the title (yes/no); all the items representing adolescents’ screen time 
(yes/partially/no); suitability of the metric (yes/partially/no); suitability of the unit of measure 
and response scale (yes/partially/no); adequacy of the sequence of items (yes/partially/no); the 
use of the bold tags on the questions to emphasize primary information on the online 
questionnaire (yes/partially/no). The experts were able to provide comments on each item and 
suggest the addition and deletion of items.  

This step was conducted to test if the target population understands the questions and 
response scales proposed (Borsa et al., 2012), as well as, ambiguity and misinterpretation of the 
items, and possible difficulties (Presser et al., 2004). Based on that, a convenience sample of 
14 high school students of a Federal Institute of Technological Education from Santa Catarina 
state participated in the reviewing of the QueST. This step involved an online questionnaire, 
which comprised the QueST and additional questions about (i) the clarity of each item (highly 
clear/quite clear/somewhat clear/not clear); (ii) unfamiliar words in each of the items (no/yes, 
which one?); (iii) if students did understand how to answer the QueST (I did/I did not 
understand); (iv) if students had any difficulty in answering the QueST (no/yes, which one?); 
and (v) if other activities involving the usage of electronic screens were lacking on the 
questionnaire (no/yes, which one?). This procedure was performed in a classroom, during 
school hours, and students accessed the electronic link of the questionnaire using their 
smartphones. 

To test the reliability of the QueST, all high school students from the Aplicação school 
were recruited, and those who agreed to participate were asked to answer the QueST twice with 
a seven days interval between applications (de Souza et al., 2017). This procedure was 
performed in a classroom, during school hours, and students accessed the electronic link of the 
questionnaire using their smartphones. The measurement conditions were similar for both test 
and retest (administrators, environment, instructions). 

The five items of the QueST were evaluated on clarity and representativeness using 
the Content Validity Index for Items (I-CVI) (Polit & Beck, 2006). The I-CVI were calculated 
by summing the ratings of either “3” or “4” in each item, divided by the total number of experts. 
Also, the Content Validity Index for Scales (S-CVI) was obtained by the arithmetic mean of 
the I-CVIs (Polit & Beck, 2006), separately calculated for clarity and representativeness. The 
authors MTGK, BGGC, and PCS analyzed the qualitative comments provided by the experts, 
and suggestions were accepted/rejected with the consensus of these three authors after revision 
and discussions. This step was blinded to secure the identity of the experts and mitigate bias.  

The information regarding the review of the QueST by the students was descriptively 
presented by proportions. Any ratings "somewhat clear" or "not clear" on the wording of any 
item, as well as any student who answered that had not understood how to answer the QueST 
was adopted as the criterion of reformulating the item or the entire instrument, entailing a 
second evaluation by the students. Furthermore, the authors MTGK, BGGC, and PCS, by 
consensus, would replace possible unfamiliar words with simpler ones. Also, possible 
suggestions for other activities made with screen media devices would be evaluated to compose 
the questionnaire. The difficulties in answering the QueST were described.   

Only students who answered both measures (test and retest) were included in the 
reliability analysis. Students with missing data were excluded. Also, implausible answers were 
excluded by adopting >14 daily hours as a cutoff value. For stability, differences between the 
test and retest were analyzed using Students t-tests. As some variables were skewed, additional 
non-parametric tests (Sign-Rank tests) were conducted to confirm the findings. The stability of 
the constructs was discerned through intraclass correlation coefficients (ICC). Also, the Bland-
Altman dispersion analyses were used for examining the differences and limits of agreement 
(in minutes) between test and retest measurements. 

Of the 24 invited experts, 16 (66.7%) submitted their answers. Eight experts did not 
answer the questionnaire, but they did not comment on the reason. Table 2 shows the I-CVI and 
S-CVI values for clarity and representativeness of the QueST. Regarding clarity, the smallest 
I-CVI was observed in Item 1 (studying): 0.88 (or 88% of agreement among the experts). Items 
2, 4, and 5 obtained I-CVI = 0.94; and Item 3 (watching videos) demonstrated 100% agreement 
among the experts. The calculated S-CVI indicated 94% of overall clarity of the QueST. 
Concerning representativeness, four out of the five items were considered as 100% 
representative (playing video games: item 4 I-CVI = 0.88), and the S-CVI indicated 
representativeness of 98%.     

Based on the review of the experts, the title of the questionnaire was modified; some 
terms in the items were replaced or added (example: to watch “sports” was added in the third 
item); the response scale was simplified, where the experts proposed a shorter scale with breaks 
of 10 minutes (0, 10, 20 minutes…), instead of a longer minute-by-minute scale. Experts also 
contributed to reordering the items to reduce mental effort. There was no addition or exclusion 
of items.      

Fourteen students (18.2±1.0 years old, 42.9% female) participated in the first review 
of the QueST. All students considered the wording of the questions to be highly or quite clear 
(Item 1: 71.4% highly clear, and 28.6% quite clear; Item 2: 78.6% highly clear, and 21.4% quite 
clear; Item 3: 85.7% highly clear, and 14.3% quite clear; Item 4: 84.6% highly clear, and 15.4% 
quite clear; Item 5: 84.6% highly clear, and 15.4% quite clear). There were no "somewhat clear" 
or "not clear" ratings. No student reported issues regarding the vocabulary, and 100% of them 
understood how to answer the QueST. Eleven students (78.6%) did not express any difficulty 
in answering the QueST; however, the other three students commented that they had difficulty 
in precisely reporting their habitual screen time. Based on the review of the students, no 
modifications to the QueST were necessary. 

From 203 eligible students, 104 students agreed to participate, provided written 
informed consent forms, and answered the QueST in both test and retest (16.3±1.02 years old; 
66.3% girls). The mean time of social media usage on a weekday was higher at test, whereas 
studying on weekend days was higher at retest. However, time watching videos on weekend 
days was higher at test compared to retest (Table 3). 

All ICC values were statistically significant (Table 4). The highest ICC was observed 
for the use of social media on weekdays (ICC= 0.76, 95% CI 0.66; 0.83), whereas the lowest 
ICC was observed in the construct of watching videos on weekends (ICC= 0.24, 95% CI 0.04; 
0.41). 

The Bland-Altman analyzes for the QueST constructs are presented in Table 5. Mean 
differences ranged from -4.6 (Upper limit: 149.6; Lower limit: -158.7) minutes for working on 
weekdays to 40.6 (Upper limit: 400.0; Lower limit: -318.9) minutes for watching videos on 
weekend days.  

The QueST proved to be adequate to evaluate different screen time constructs with 
satisfactory content validity. However, the stability of the items varied considerably across the 
constructs and the days analyzed (weekday versus weekend). The content validity was 
considered appropriate based on the level of agreement among experts for clarity and 
representativeness of the items and the instrument. According to the acceptability criteria of the 
items that incorporate the standard error of the proportion of agreement, the lowest I-CVI 
admitted is 0.78 in a panel of experts with 6 or more individuals (Lynn, 1986; Polit & Beck, 
2006). Also, the instrument as a whole has acceptable content validity when the S-CVI is ≥0,90 
(Waltz et al., 2005). In addition, the comments/suggestions given by experts were 
complementary to the validation process. This step contributed to some textual modifications 
and minor additions in the instrument, which was not robust enough to require another 
submission to the panel of experts. Overall, the QueST can be used to assess screen time in 
adolescent populations. 

The review of the instrument according to the target population and experts is strongly 
recommended (Hidding et al., 2017; Mokkink et al., 2010). However, this process was not 
reported by more than 80% of studies examining the measurement properties of sedentary 
behavior instruments (Hidding et al., 2017). Regarding the initial review by adolescents, the 
QueST was considered comprehensible and clear. However, three students reported difficulty 
in accurately reporting the usual screen time in each of the items. This problem is common in 
obtaining accurate memories in questionnaires to measure behaviors with children and 
adolescents (Kohl et al., 2000). The screen time may be variable and unstable over time and are 
dependent on several factors (Cabanas-Sánchez et al., 2018), which may contribute to poor 
estimation of habitual behaviors. To improve this estimation, the response scale was updated, 
making it less arduous for adolescents to understand the items and report their behavior. 

The stability of the items ranged from poor to excellent (Rosner, 2005) and the sample 
size of this procedure was considered adequate (Terwee et al., 2007). The item for watching 
videos (series, movies, soap operas, news, sports, programs, others), both on weekdays and 
weekends, showed the lowest ICCs compared to the other items. This may be explained by the 
fact that this behavior is not stable throughout the time between the repetitions of the 
measurements as some factors can influence screen behavior even in a short time frame (Hardy 
et al., 2007). For example, the launch of a new season in a popular series, or the occurrence of 
acclaimed sporting events (e.g., Olympic games, international league finals) can considerably 
increase the electronic screen usage within a few days and inflate only one measure, either test 
or retest. Thus, the answers in the test and retest may be accurately reported by adolescents, but 
it is still verified as poor stability of the measurements because a particular behavior does not 
present “typical” or “normal day” patterns (Hardy et al., 2007). Further studies are needed to 
understand the dynamics of video watching among adolescents in periods longer and shorter 
than 1-week in order to investigate the length of the most appropriate test-retest interval to 
obtain population parameters. 

The item for playing video games presented fair and good reliability on week and 
weekend days, respectively, demonstrating considerable accuracy and stability of the responses 
to this behavior. The ICCs obtained were similar to those of the Health Behavior in School-
aged Children study (2008), which showed ICC= 0.54 (95% CI 0.38; 0.67) on weekdays and 
0.69 (95% CI 0.57; 0.78) on weekends for the gaming item (Liu et al., 2010).   

Similarly, the item about social media/chatting applications demonstrated 
good/excellent reliability on both weekdays and weekend days. Stable, but high volumes 
characterized this behavior; however, this is expected as they are predominantly realized on 
smartphones over long periods of the day (de Vries et al., 2016; Devine & Lloyd, 2012; Ihm, 
2018), possibly while multitasking (e.g., watching a movie on the television while chatting on 
the smartphone). It was also observed that the amount of time spent on social media and chat 
applications in the test measurement was statistically higher than the observed in the retest on 
weekdays. It is not possible to establish a single explanation for this difference; however, school 
responsibilities and parental controlling are examples of factors that may influence these 
activities, and consequently, impact test-retest measures over a short period.  

The item related to screen time for studying on weekdays showed fair/good stability 
and was higher compared to the ICC obtained on the weekends. Possibly, the time spent on 
studies over the weekend is more variable or flexible and determined by school demands, such 
as the proximity to exams at school compared to the time spent studying on weekdays, when 
the adolescents already have established a stable routine of school tasks. 

Also, low volumes of screen time for working on weekdays and weekends were 
reported, and these questions demonstrated fair stability. Previously, a survey conducted in the 
state of Santa Catarina, Brazil, assessing lifestyle indicators of high school students (15-19 
years old) reported that 50.5% of the adolescents had a job in 2011 (Silva et al., 2013). In 
general, screen time items related to study and work constructs are not common in sedentary 
behavior research among adolescents, as these constructs are not discretionary, and few studies 
include questions accessing this information (Hidding et al., 2017; Prince et al., 2017). 

The present findings suggested that adolescents’ screen time behaviors were less stable 
on weekend days compared to weekdays. This result may be more related to the natural 
variability of these behaviors, especially on weekends, than to the reduced reliability of the 
items. Adolescents’ screen time behaviors on weekends can be influenced by opportunities to 
practice physical activities (Hardy et al., 2007), weather conditions, and events that promote 
the use of electronic devices (e.g., the release of series or games; exams at school). Also, on 
weekends, more spontaneous and fewer routine behaviors are expected, when adolescents may 
have more free time to use electronic devices as they please. 

A “typical day” was used as the reference time frame in the items of the QueST to 
exclude atypical events on the measurements, such as decisive exams at school, because it could 
directly influence the item for studying, for example. However, possible atypical occurrences 
could not be controlled in this study. Besides, some adolescents’ screen time behaviors, such 
as watching videos, may vary highly within and between individuals, which also impairs the 
accuracy of respondents. Nevertheless, some bias may be unavoidable when behaviors are self-
reported among this population (Kohl et al., 2000). 

Among the strengths of this study, we highlight the use of a wide range of screen time 
constructs which represent a large amount of sedentary time of adolescents; the use of 
standardized and recommended methods for the development and validation of questionnaires, 
which is not documented for the majority of available sedentary behavior instruments (Hidding 
et al., 2017); the content validity focused on assessing the representativeness and clarity of the 
items using qualitative and quantitative methods. Besides that, this study sought to include the 
complete QueST content validation process, which encompassed two complementary steps: the 
review of the questionnaire by the target population and its evaluation by the field experts; 
finally, the methodological procedures of this study were adopted according to the Consensus-
Based Standards for the Selection of Health Measurement Instruments (COSMIN) (Mokkink 
et al., 2010) (see Supplementary Material: Application of the COSMIN checklist on the 
QueST). 

This study had as limitations the small sample size obtained by convenience sampling 
in the initial test (n=14); the criterion validity was absent due to the lack of a gold standard 
measure used in the free-living conditions that could be adopted as a reference for the 5-screen 
time constructs present in QueST. This step remains a challenge for research with this purpose, 
considering that these screen time behaviors can be performed on different devices (e.g., 
television, computer, tablet, smartphone); and the QueST was developed to cover the activities 
that adolescents perform using any electronic screen device in five previously established 
constructs, however, not all activities fit into a construct of the questionnaire, such as reading 
eBooks for leisure. 

The 
final 
electronic 
version 
of 
the 
QueST 
is 
available 
at 
pt.surveymonkey.com/r/QLQTQHG 
(Brazilian 
Portuguese) 
and 
pt.surveymonkey.com/r/Q7QXYL2 (English). 

The QueST presented satisfactory content validity determined by the panel of 16 
experts and adequate evaluation by the adolescents. The wide variability in reliability that was 
observed among the five items of the instrument highlights the natural fluctuation of the 
adolescent behavior in certain screen time constructs. In general, QueST can be considered an 
appropriate tool to measure adolescents' screen time in the five constructs presented. 

Authors gratefully acknowledge the Federal Institution of Education, 
Science and Technology of Santa Catarina, and the Federal University of Santa Catarina. We 
also thank all the members of the expert group, including Filipe da Costa, Rômulo Fernandes, 
Evelyn Ribeiro, Adriano Hino, Grégore Mielke, Marcelo Romanzini, Leandro Rezende, 
Adriano Borgatto, Jeffer Sasaki, Paulo Guerra, Cassiano Rech, Leandro Garcia, Diego 
Christofaro, Valter Barbosa Filho, Anelise Gaya, and Andreia Pelegrini, for their contribution 
to this research instrument. 
