In the initial round of coding, all raters analyzed the same group-talk episode and engaged in a discussion regarding the alignment and convergence scheme and its associated rules. Following the attainment of moderate scores, the coding scheme was refined by eliminating certain codes, and the rules were clarified and elaborated upon. Two raters subsequently coded 28% of the episodes (12 out of 42), and inter-rater reliability was assessed using Cohen’s Kappa scores calculated through R (RStudio Team, 2020). This iterative process resulted in an acceptable to excellent level of agreement, with Cohen’s Kappa scores averaging K = .79 (Kappa values for each code are detailed in Table 1). Finally, the researcher coded the remaining material, thereby ensuring the reliability of the coding scheme.