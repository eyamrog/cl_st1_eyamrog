The current study delves into the Dimensions and Indicators of the academic international multidimensional ranking known as "U-Multirank." This examination holds particular relevance in the contemporary Brazilian context, given that the new evaluation of graduate programs in Brazil incorporates various aspects of multidimensional assessment, drawing on concepts utilized by U-Multirank. This ranking delineates the strengths and weaknesses of each Academic Institution through five Dimensions comprising 36 Indicators. While this extensive array of Dimensions and Indicators offers a comprehensive perspective of Universities, it also prompts inquiries regarding their autonomy and the availability of data. The analysis focuses on the top 300 European Academic Institutions featured in the 2020 edition, elucidating the Dimensions and Indicators while establishing statistical correlations among them.
The issue of academic evaluation has long been a focal point in the field of education. In Brazil, assessments have been a longstanding practice in higher education, as noted by Guimarães and Esteves (2018). Specifically concerning graduate programs, the Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES) has been evaluating these programs for several decades, as highlighted by Rodrigues et al. (2020). Currently, there is a notable shift in the criteria being examined, with a move towards incorporating concepts from a multidimensional approach.
In the realm of international academic evaluations, numerous renowned rankings exist (Calderón, França, & Gonçalves, 2017; Calderón & França, 2018; Dill & Soo, 2005; Eccles, 2002; Ganga-Contreras et al., 2020), emerging at the onset of the 21st century and progressively gaining prominence. Presently, they are deemed crucial for nearly all academic institutions worldwide, serving as an official "seal of quality" for those securing favorable positions in these rankings, thereby enhancing their ability to market their offerings in the competitive global education landscape. Academic rankings have evolved into a distinct field of research, evident in the extensive array of publications available (Billaud, Bouyssou, & Vinke, 2010; Calderón & França, 2018; Gonçalves & Calderón, 2017; Herting, 2016; Liu & Cheng, 2005; Marginson & Van der Wende, 2007; Bernhard, 2012; Shin et al., 2011; Sorz et al., 2015; Stack, 2016; Van Raan, 2005; Webster, 2001; Aguillo et al., 2010; Aguillo et al., 2006; Théry, 2020).
The concept of multidimensional evaluation in academic institutions, as currently embraced by CAPES, is not a novel notion, having originated in Europe in 2008 with the proposal of a multidimensional ranking system by Van Vught and Ziegele (2012). The selection of Dimensions and Indicators holds significant importance, alongside an initial assessment of the interdependence among these elements. CAPES presently emphasizes the "Research" Dimension within the "U-Multirank," prompting an exploration of the correlations between this Dimension and others to provide a preliminary insight into the potential outcomes of a multidimensional evaluation within the Brazilian context.
The rationale behind the development of "U-Multirank" stemmed from the recognition that there is no definitive "best" or universally applicable top 10 ranking of academic institutions worldwide. The notion that the "best" university is a subjective determination made by individual students, contingent upon their specific goals and limitations, underpins this approach. Consequently, a multidimensional ranking system aims to delineate key criteria (referred to as Dimensions) and the methodology for assessing these criteria (comprising Indicators and the corresponding calculation protocols). Institutions are categorized into five quality groups for each Indicator based on predefined criteria. Rather than a singular ranking, classifications are established for each Indicator, allowing users to focus on the Indicators most pertinent to their needs and preferences.
The "U-Multirank" (https://www.umultirank.org/; PRADO 2021a and 2021b) was developed with the aim of evaluating the performance of educational institutions across five dimensions: Teaching and Learning, Research, Knowledge Transfer, International Orientation, and Regional Engagement. Each of these dimensions is further subdivided into a varying number of indicators, ranging from four to eleven, depending on the specific dimension. This tool caters to prospective students seeking an international ranking of higher education institutions to aid in selecting an institution that aligns closely with their interests. Users have the flexibility to assess individual indicators independently or in grouped families, focusing on those deemed most critical for their decision-making process.
The "U-Multirank" stands out due to its extensive array of indicators, totaling 36, a stark contrast to the typically limited number of indicators, usually less than 15, found in one-dimensional rankings. This abundance of indicators raises concerns regarding potential data gaps, which could be perceived as a drawback of this ranking system. However, unlike one-dimensional rankings that necessitate a comprehensive dataset for accurate classification, the U-Multirank allows users to overlook missing information and focus on available data, albeit at the cost of losing some insights. While this flexibility diminishes the significance of data gaps, the prevalence of missing data in various scenarios remains a noteworthy issue that this paper will address in subsequent sections.
Another key focus of this study is to determine whether institutions tend to prioritize specific dimensions and indicators over others, or if they exhibit a consistent approach across all dimensions. This analysis will specifically target the top 300 performing institutions in Europe, as these institutions typically possess more comprehensive datasets, thereby reducing the impact of missing data issues, particularly within the European context.
In conducting this study, the initial step involves defining the concept of "best performers" within the framework of "U-Multirank". This task is not straightforward due to the ranking's specific objective of steering clear of a broad categorization, as previously elucidated. The Dimensions encompass varying quantities of Indicators, with several Indicators exhibiting missing data. When seeking an overall classification of Academic Institutions or identifying the "best performers" within a particular country, "U-Multirank" bases its classification primarily on the number of "A" grades (the highest grade) allocated to an Institution. Other grades come into play only in instances of ties among two or more Institutions. This approach raises concerns as it minimizes the distinction between "B", "E", or unreported data. In this paper, two alternative methods for establishing a general ranking are considered. The first method involves calculating the average of all 36 Indicators assessed by "U-Multirank". While this approach acknowledges all available data, thereby recognizing Institutions' efforts in data reporting to enhance their scores, even if they fall short of an "A" grade, it is not without drawbacks. The varying number of Indicators across Dimensions, ranging from four to eleven, results in Dimensions with more Indicators carrying greater weight in the final ranking. To address this issue, a second proposal is put forth. Initially, the average is computed within each Dimension, followed by the average of the five grades assigned to the Dimensions. While there is no flawless method for constructing a comprehensive ranking using "U-Multirank", a necessity occasionally fulfilled by the ranking system itself, this paper opts for the latter approach to identify the top 300 performers in Europe in 2020, as detailed in Appendix 1, as it accords equal importance to each Dimension.
After compiling a list of institutions, the correlations among all pairs of indicators within the same dimension are computed and analyzed. Similarly, the correlations among the averages of each dimension are examined. These correlations serve the purpose of investigating the interrelation between dimensions and indicators, shedding light on the degree of independence among them. It is anticipated that certain indicators will exhibit strong correlations. For instance, indicators such as the absolute number of publications, the normalized number of publications, and the count of "top cited" publications are likely to display high correlations. Institutions with elevated values in one of these indicators are expected to demonstrate high values across all of them. Assessing these correlations is crucial as they offer insights into whether the study effectively captures the essence of the five dimensions and 36 indicators, or if some indicators merely represent different approaches to measuring the same underlying aspect through varied inquiries.
In this study, we aim to investigate the statistical correlations among various Dimensions to determine whether Institutions exhibit consistent performances across all Dimensions or if they show a stronger emphasis on specific ones. For instance, we will explore whether top performers in "Teaching and Learning" also excel in "Research" or any other Dimension. Additionally, we find it intriguing to examine the relationships between Dimensions, particularly since CAPES' conventional evaluation primarily emphasizes the "Research" Dimension. Understanding how this Dimension typically interacts with others in multidimensional rankings can provide valuable insights into the overall performance of Institutions.
In the 2000s, international rankings for academic evaluation began to surface, aiming to pinpoint academic institutions deemed as "World Class Institutions." The inaugural international academic ranking, the "Academic Ranking of World Universities (ARWU)" or the "Shanghai Ranking," was established in 2003 by the University of Shanghai, China, as noted by CALDERÓN and FRANÇA (2018). This ranking's primary objectives were to furnish data for the Chinese government to identify international educational institutions for sending Chinese students abroad and to assess the standing of Chinese institutions against global benchmarks.
Inspired by this initial ranking, various international rankings emerged, including the "Webometrics Ranking of World Universities" (http://www.webometrics.info/en; AGUILLO, ORTEGA, and FERNANDEZ, 2008) in 2004, and the ranking "THE-QS" in the same year, which later split into the "Times Higher Education World University Rankings" (known as "THE") (https://www.timeshighereducation.com/world-university-rankings) and the "QS World University Rankings" (known as "QS") (https://www.topuniversities.com/university-rankings) in 2010. Building on this momentum, numerous other countries developed regional or national rankings, particularly due to the fact that international rankings often overlook smaller and local academic institutions in various regions worldwide (RIGHETTI, 2019; SHIN and TOUTKOUSHIAN, 2011).
The concept of a multidimensional ranking system was first introduced at a conference in 2008 during the French Presidency of the European Union by Van Vught and Ziegele (2012). This initiative stemmed from the recognized need for a novel methodology to assess the diverse dimensions of quality within higher education institutions. Subsequently, this concept led to the development of "U-Multirank," which currently encompasses 1,759 universities across 92 countries in its 2020 version. This comprehensive ranking includes approximately 5,000 faculties and over 11,400 courses spanning 28 subject areas (https://www.umultirank.org/about/u-multirank/frequently-asked-questions/).
The "U-Multirank" was not intended to create a comprehensive classification of educational institutions but rather focuses on specific performance indicators grouped into dimensions. This approach allows the ranking to highlight institutional strengths and weaknesses within each indicator and dimension. Users have the flexibility to create their own rankings by selecting dimensions and indicators that align with their priorities. The multidimensional nature of this ranking system is often justified by the argument that one-dimensional rankings lack robustness. Small alterations in indicator weights can lead to significant shifts in results, undermining the validity of such simplistic rankings.
In the "U-Multirank," institutions are categorized into five performance groups for each indicator: A (Very good), B (Good), C (Average), D (Below average), and E (Weak). This classification aims to mitigate accuracy issues by grouping institutions, which helps filter out small differences that fall below the measurement accuracy. Consequently, accuracy problems primarily arise at the boundaries between performance groups, effectively minimizing the issue. As a result of this grouping strategy, a significant number of institutions end up with the same grades.
An examination of "U-Multirank" reveals that indicators sourced independently of the institutions being assessed, such as publication numbers and citations, are universally accessible across educational institutions and exhibit a high degree of reliability. Conversely, data like graduates' workplace locations and graduation times, acquired through questionnaires distributed by educational institutions and students, are frequently unavailable and lack precision. Additionally, certain information is marked as "Not-Applicable," such as the quantity and job placements of Master's program graduates in institutions that do not offer such programs.
Figure 1 illustrates the classical view summarizing the outcomes of "U-Multirank," accessible at https://www.umultirank.org/export/sites/default/press-media/media-center/universities/2020/country-reports/UK-Country-report-2020.pdf. The circle delineates the five Dimensions of the rank: Teaching and Learning (green), Research (pink), Knowledge Transfer (blue), International Orientation (orange), and Regional Engagement (purple). Each dimension is further subdivided into specific indicators. For Teaching & Learning, these include Bachelor graduation rate, Masters graduation rate, Graduating on time (bachelors), and Graduating on time (masters). Research indicators encompass External research income, Research publications (size-normalized), Art related output, Citation rate, Top cited publications, Interdisciplinary publications, and Post-doc positions. Knowledge Transfer indicators consist of Income from private sources, Co-publications with industrial partners, Patents awarded (size-normalized), Industry co-patents, Spin-offs, Publications cited in patents, and Income from continuous professional development. International Orientation indicators cover Foreign language bachelor programs, Foreign language master programs, Student mobility, International academic staff, International doctorate degrees, and International joint publications. Regional Engagement indicators include Bachelor graduates working in the region, Student internships in the region, Regional joint publications, Income from regional sources, and Master graduates working in the region. For a comprehensive understanding of the calculations behind each indicator, refer to the Indicator book of "U-Multirank" at https://www.umultirank.org/export/sites/default/press-media/documents/Indicator-Book-2020.pdf.
Upon comparing the tables generated by the site, it becomes evident that seven out of the 36 indicators displayed in tabular form are not represented graphically. These indicators include Research publications (absolute numbers), Strategic research partnerships, Professional publications, Open access publications, Patents awarded (absolute numbers), Graduate companies, and Regional publications with Industrial partners.
Each bar representing the indicators is divided into five segments and colored dark to signify the grade received for that indicator. A completely dark bar indicates an "A" grade, whereas a completely light bar signifies "No data available."
The classification methodology employed by "U-Multirank" resembles that of an "Olympic Medals Table," where institutions with the highest number of maximum scores (A) are considered top performers. Scores below A, such as B and below, are only utilized for tiebreakers. Consequently, an institution with 20 A scores and 16 E scores would rank higher than one with 19 A scores and 17 B scores. This approach raises concerns as it minimally distinguishes between B and E scores. In certain scenarios, institutions might strategically focus on a select few indicators while neglecting others to boost their ranking, potentially compromising the quality of services provided to students in certain areas.
The current study introduces two additional rules to formulate a comprehensive ranking that considers all grades. The first rule involves calculating the simple average of all the indicators provided. While this method incorporates all available data, it assigns equal weight to each indicator, failing to proportionally emphasize all dimensions involved. This lack of proportional importance arises from the varying number of indicators, ranging from four to eleven, depending on the dimension. Consequently, certain dimensions would carry significantly more weight than others in determining the final grade.
In order to address this issue, an alternative method for conducting a comprehensive assessment is suggested. This approach involves initially calculating the averages within each Dimension, followed by determining the overall average by incorporating the grades from each Dimension. Equal weighting is assigned to each Dimension, while varying weights are assigned to the Indicators.
To illustrate the impact of adhering to the criteria outlined by "U-Multirank," Figure 2 presents the average of the Dimensions on the vertical axis against the Academic Institution's position according to "U-Multirank" on the horizontal axis. This analysis encompasses all 1070 European Academic Institutions featured in the 2020 edition. While there is a general trend of higher averages for top-performing institutions, the correlation is not particularly robust, as evidenced by Figure 2. Surprisingly, numerous institutions with ratings exceeding 2.5 find themselves positioned within the bottom 300 ranks in the overall ranking. This discrepancy underscores the necessity for a cautious approach towards the guidelines established by "U-Multirank," suggesting a potential need for revision.
To delve further into this aspect, Figure 3 illustrates the outcomes of the three approaches employed for general classifications. The horizontal axis delineates the ranking position derived from "U-Multirank," exclusively utilizing the "A" numbers. On the vertical axis, the institution's position is depicted by blue dots from "U-Multirank," red dots from the average of Indicators, and green dots from the average of Dimensions. This analysis encompasses 1070 European academic institutions featured in the 2020 edition of "U-Multirank." European countries were specifically chosen due to their more comprehensive databases in comparison to other continents.
The disparities in positions are notably significant. Statistical correlations were computed for each pair of classifications, yielding the following results: "U-Multirank" versus the average of the Indicators at 0.8035; "U-Multirank" versus the average of the Dimensions at 0.6179; and the average of the Indicators versus the average of the Dimensions at 0.8403. This suggests that utilizing the averages of Indicators and Dimensions yields the most coherent results, while adhering to the criteria outlined by "U-Multirank" produces less correlated outcomes, especially when compared to results derived from the average of Dimensions. The proposition of employing the average of Dimensions appears to be the most rational approach for a comprehensive ranking, hence it is adopted here to identify the "top 300 performers" in Europe for the year 2020. Naturally, the impact of missing or "Not Applicable" data is evident across all classifications. In the classification by "U-Multirank," it does not contribute to an "A," whereas it is considered as zero in classifications utilizing the average of Indicators or the average of Dimensions. The repercussions are more pronounced in the latter two classifications, yet this study deems it equitable to penalize institutions that fail to provide data, which is the sole reason for the absence of data. Instances of "Not Applicable" data are relatively minor and do not lead to alterations significant enough to sway the conclusions drawn from the statistical analyses conducted in this study.
To calculate the average of Dimensions in the "U-Multirank," the initial step involves computing the average for each Dimension individually. Subsequently, examining the distribution of these averages becomes crucial. For a preliminary overview, Figure 4 is constructed, where the horizontal axis represents the average of Dimensions for each Institution. On the vertical axis, the averages for Teaching and Learning (depicted by blue diamonds), Research (illustrated by red squares), Knowledge Transfer (represented by green triangles), International orientation (shown as purple circles), and Regional Engagement (displayed as blue X) are plotted. This analysis encompasses the 1070 European academic institutions included in the 2020 edition of the "U-Multirank."
A cloud of dispersed points exhibiting a positive correlation is observed, indicating that institutions with higher average dimensions tend to have higher grades across individual dimensions. While this outcome is anticipated, Figure 4 provides a more detailed representation of these trends. Vertical lines are prominently visible, delineating the range of grades within each dimension for a specific average of dimensions. These lines exhibit significant magnitude, often approaching 2 units, which represents half of the total interval displayed, given that the averages fall within the range of zero to 4.0.
These findings highlight significant variations in the average performance of Dimensions across Academic Institutions, demonstrating a lack of uniformity in their achievements across all Dimensions, even within European countries. This suggests that creating a broad classification is not advisable as it would obscure the diverse performances observed. There is no justification for deeming one Dimension superior to others, particularly in a general sense.
Next, a more comprehensive investigation is undertaken regarding the quantity of missing and accessible data within "U-Multirank," along with an examination of the data's origins, distinguishing between information sourced from open platforms and data provided by institutions. The objective is to enhance our comprehension of each indicator, pinpointing those that serve as less reliable or more robust metrics for ranking purposes, taking into account their sources and accessibility. Once more, we will utilize the dataset pertaining to the top 300 performers in European nations as per the 2020 edition of the ranking.
Table 1 presents the indicators utilized by "U-Multirank," categorized into five dimensions. It provides a description of each indicator, the quantity of available data, unavailable data, and data deemed as "not applicable," along with the data source divided into two categories: "IQ," denoting data derived from institution-filled questionnaires, and "IND," indicating data sourced from independent entities such as the Web of Science.
Examining the broader scope, among the 300 leading European academic institutions featured in the 2020 ranking, an anticipated total of 10,800 evaluations is projected, factoring in the presence of 36 distinct indicators. Regrettably, the available data stands at 9,258, representing 85.72% of the expected figures. Concurrently, there are 1,102 missing data points, accounting for 10.20%, and an additional 440 entries (4.08%) categorized as "not applicable." This signifies a notable 15% of missing data within this specific cohort of academic institutions, a statistic that warrants attention.
The distribution of missing data is non-uniform, resulting in varying levels of completeness across different Indicators. Figure 5, derived from Table 1, visually represents the disparity in data availability per Indicator. A closer examination of this graphical representation highlights a significant imbalance in the availability of data among the various Indicators.
The initial observation highlights the presence of two indicators that exhibit significant weakness in terms of data availability, both falling below the threshold of 100 (33%) data points: "Strategic research partnerships" with entirely missing data, and "Industry co-patents" with 91 recorded grades, 5 missing values, and 204 instances marked as "Not Applicable." Given the presence of 34 other indicators with more comprehensive data sets, these two indicators will be excluded from the statistical analyses moving forward to prevent the undue influence of a substantial number of unavailable data points.
In terms of the remaining indicators, one falls below 50% and above 33% of available data, specifically "Graduate companies." Another indicator, "Art related output," falls within the range of 50% to 66% of data availability. Additionally, two indicators, "Professional Publications" and "Spin-Offs," have data availability falling within the range of 66% to 75%. Consequently, there are a total of 30 indicators with more than 75% of data available, accounting for 83.33% of all indicators considered in the study.
In terms of Dimensions, we have data availability rates of 95.83% for "Teaching and Learning," 81.42% for "Research," 95.17% for "Knowledge Transfer," 95.17% for "International Orientation," and 84.83% for "Regional Engagement." Notably, "Teaching and Learning" and "International Orientation" emerge as the most dependable Dimensions, boasting percentages exceeding 95%. However, the remaining Dimensions also present acceptable figures, all surpassing the 80% threshold.
Table 2 presents the data acquired from both independent sources and Academic Institutions. Out of the 36 total Indicators, 22 (61.11%) were obtained through questionnaires, while 14 (38.89%) were sourced independently. This distribution underscores a significant reliance on data provided by Academic Institutions, highlighting the crucial role of a comprehensive database. The effectiveness of "U-Multirank" is shown to vary across different geographical regions due to substantial disparities in data availability from country to country. Examining each Dimension individually, it is evident that "Teaching and Learning" lacks data from open sources entirely, while "Research" (54.55%) and "Knowledge Transfer" (55.56%) draw slightly over half of their data from open sources. In contrast, "International Orientation" relies on open sources for only 16.67% of its data, and "Regional Engagement" for 33.33%. Consequently, there is a clear need for increased efforts to encourage Institutions to provide their data in a more comprehensive manner.
Delving into the definitions of the Indicators reveals their highly objective nature, characterized by clear definitions and reliance on numerical data governed by pre-established rules. Unlike rankings that incorporate subjective elements such as reputation, where judgments are influenced by personal opinions and experiences within the international academic and industrial communities, the Indicators in question maintain a strict focus on quantifiable metrics. This distinction sets them apart from traditional one-dimensional rankings. For instance, renowned rankings like the "Times Higher Education World University Rankings-THE" and the "QS World University Rankings-QS" allocate significant weight, 33% and 50% respectively, to reputation-related indicators in their final assessments. This underscores a key strength of the "U-Multirank." However, despite its objectivity, a substantial portion (61.11%) of the data is reliant on information provided by the Institutions themselves, raising concerns about data accuracy and completeness. This highlights an area in need of improvement within the framework of the ranking system.
The subsequent focus of this study pertains to the examination of statistical correlations among various Indicators and Dimensions within the "U-Multirank" framework. This analysis holds significant importance as high correlations between multiple Indicators could indicate redundancy in the measurement process, suggesting that some of the 36 Indicators may be measuring similar aspects through different questions. Such redundancy could potentially undermine the credibility of the "U-Multirank" results and its touted multidimensional nature, which stands as a key distinguishing feature of this ranking system.
These measurements are instrumental in forecasting the behaviors of institutions when assessed through multidimensional criteria, shedding light on whether the institutions under scrutiny exhibit homogeneous or heterogeneous performances across various dimensions.
The study utilized the top "300 performers" in Europe from the 2020 edition of "U-Multirank." This sample selection was based on their possession of a more comprehensive dataset, as previously elucidated. Additionally, these institutions share the advantage of being situated on the same continent, thereby minimizing potential influences stemming from disparate cultures and other unique characteristics. This alignment allows for a concentrated examination of the Indicators and Dimensions employed by the ranking system.
The primary statistical tool utilized for the analysis in this study is the correlation coefficient, which is formally defined as: <>
In the equation presented, X and Y represent the values of the two variables being analyzed, while n denotes the number of data pairs. The resulting value falls within the range of -1 to 1, where -1 indicates a 100% negative correlation, signifying a linear relationship where one variable increases as the other decreases. Conversely, a value of 1 signifies a 100% positive correlation, indicating a direct linear relationship where both variables increase together. A value of zero suggests complete independence between the variables, denoting no relationship between the quantities. Intermediate values indicate partial relationships, either positive or negative. For this study, the following interpretations are applied to these correlation values: -0.19 to 0.19: very weak correlation; -0.20 to 0.39: weak correlation; -0.40 to 0.69: moderate correlation; -0.70 to 0.89: strong correlation; -0.90 to 1.00: very strong correlation. The initial findings are detailed in Table 3, illustrating the correlations among the five Dimensions of the U-Multirank for the "Top-300 performers" in Europe in 2020.
The analysis reveals that there are no significant "strong" or "very strong" correlations among the Dimensions, with only two instances of "moderate" correlations being evident: a positive correlation between "Research" and "Knowledge Transfer" and a negative correlation between "Research" and "Regional Engagement". This finding is advantageous for the ranking as it indicates a potential for five distinct and independent Dimensions within the study. To provide a clearer visualization of these "moderate" correlations, Figure 6 illustrates the distribution of grades for both pairs: a) presenting the data for "Research" and "Knowledge Transfer" (Correlation of 0.5509) and b) displaying the data for "Research" and "Regional Engagement" (Correlation of -0.4175). These plots exemplify typical patterns associated with moderate positive and negative correlations.
The true significance of these "moderate" correlations lies in the tendency for institutions to exhibit similar performances in "Research" and "Knowledge Transfer," while demonstrating contrasting performances in "Research" and "Regional Engagement."
While the correlations are considered weak, it is noteworthy that negative numbers exceeding 0.2500 in magnitude are observed for the pairs of Dimensions "Teaching and Learning" and "Research", "Teaching and Learning" and "Knowledge Transfer", as well as "Knowledge Transfer" and "International Orientation". This indicates a certain degree of contrasting performances within each pair of Dimensions, drawing attention to potential relationships worth exploring further.
Similarly, noteworthy are the positive "weak" correlations exceeding 0.2500 between the Dimensions "Research" and "International Orientation," as well as "Knowledge Transfer" and "International Orientation," indicating a degree of similarity in their performances.
In summary, the Dimensions exhibit low correlation, indicating that they capture distinct facets of Academic Institutions and do not demonstrate uniform performance across all Dimensions. An institution may excel in one Dimension while performing less impressively in others.
The autonomy of Dimensions underscores the significance of a multidimensional ranking system that is not intended for overarching categorizations of Academic Institutions. A broad classification approach would amalgamate Dimensions and Indicators that lack correlation, thereby obscuring the distinct weaknesses and strengths of individual Institutions.
The current study focuses on examining the correlations among the indicators within each dimension as measured by "U-Multirank." Specifically, the analysis centers on the dimension of "Teaching and Learning," with the results presented in Table 4.
Table 4 illustrates that there are no significant correlations classified as "strong" or "very strong" among the Indicators, with only one "moderate" correlation identified - a positive relationship between "Graduating on time (bachelors)" and "Graduating on time (masters)". This suggests that Institutions ensuring timely graduation for students do so consistently across both Bachelor's and Master's levels, displaying a moderate correlation between them. The remaining correlations are deemed "weak", indicating that within the Dimension of "Teaching and Learning", despite comprising only four Indicators, there exists a notable level of independence, rendering them suitable for assessing this Dimension. In summary, the average magnitude of all Indicators within this Dimension is calculated at 0.2005, with a standard deviation of 0.2443, marking the lowest average across all Dimensions. These findings affirm the validity of this Dimension within the scope of the Institutions under study.
Next, the Dimension of "Research" is examined. Table 5 displays the correlations among the Indicators associated with this Dimension.
The findings reveal a singular "very strong" correlation between the Indicators "Citation Rate" and "Top Cited Publications," boasting a correlation index of 0.9402, signifying a nearly flawless positive relationship. This suggests that Institutions with higher citation rates also tend to have the most frequently cited publications. While this correlation is not unexpected, the substantial value of this index implies a close alignment in rankings between the two Indicators, rendering them somewhat redundant.
The second highest correlation coefficient observed is between "Research publications (absolute numbers)" and "Research publications (size-normalized)", yielding a value of 0.6953, which falls just within the threshold for a "strong" correlation. This suggests that while the rankings of total publications and publications per faculty member are similar, they are not identical. Arguably, the metric of publications per faculty member may serve as a more reliable indicator, as comparing raw publication numbers across institutions of varying sizes may not be entirely appropriate. Nonetheless, the differences in results between the two metrics are not substantial.
Table 5 displays eleven additional "moderate" correlations. On average, the magnitude of all indicators within this dimension is 0.3515, with a standard deviation of 0.1763. While this average is the highest among all dimensions, it does not reach a level that would render the indicators redundant. Consequently, these results affirm the validity of this dimension within the group of institutions examined in this study.
Some intriguing correlations have been observed in the study. The Indicator "External Research Income" shows no significant or very strong correlation with any other Indicator, suggesting that external income does not necessarily lead to increased publications, citations, or art-related products. Similarly, the presence of "Post-doc" positions does not seem to have a significant impact on these production outcomes.
We now shift our focus to the dimension of "Knowledge Transfer." Table 6 displays the correlations among the indicators associated with this dimension.
The findings indicate that while there is no "very strong" correlation, two "strong" correlations exist. The initial correlation is observed between the indicators "Patents awarded (absolute numbers)" and "Patents awarded (size-normalized)", with a correlation index of 0.8077. This suggests that institutions excelling in producing new products also perform well in both indicators, indicating a high level of redundancy between them.
The second notable correlation exists between the indicators "Co-publications with industrial partners" and "Spin-offs," indicating a strong connection between industrial outputs. This correlation is quantified by a correlation index of 0.7197. Additionally, these indicators exhibit a high level of redundancy.
There are four "moderate" correlations evident in Table 6. The average magnitude of all the indicators within this dimension is 0.2722, with a standard deviation of 0.1985. These statistics indicate that this dimension comprises a strong set of indicators, with only one redundancy among the seven indicators considered within the group of institutions under examination.
We now shift our focus to the dimension of "International Orientation." Table 7 displays the correlations among the indicators associated with this particular dimension.
The findings indicate that there are no "very strong" or "strong" correlations among the indicators within this dimension, with only two displaying "moderate" correlations. Consequently, the indicators within this dimension exhibit a high level of independence from each other. The average magnitude of all indicators within this dimension is calculated to be 0.2451, with a standard deviation of 0.1278, thereby confirming the validity of this dimension for the institutions included in this study.
In the subsequent analysis, we delve into the Dimension of "Regional Engagement," as detailed in Table 8. The findings reveal a lack of a "very strong" correlation, yet a "strong" correlation is evident among the Indicators within this Dimension. Specifically, a notable correlation of 0.7668 emerges between the Indicators "Bachelor graduates working in the region" and "Master graduates working in the region," a correlation that is not unexpected given the similarity of these Indicators. This suggests that Institutions witnessing a substantial number of graduates employed within the region experience this trend across both bachelor's and master's levels. Additionally, a solitary "moderate" correlation is observed, indicating a high degree of independence among the Indicators within this Dimension, save for this one exception. The average magnitude across all Indicators within this Dimension stands at 0.2483, with a standard deviation of 0.2001, thereby affirming the validity of this Dimension within the cohort of Institutions under examination.
Table 9 presents a summary of the averages and standard deviations of the magnitudes of the Indicators for each Dimension, indicating good levels of independence among them. The maximum average correlation coefficient of 0.3515 is observed for the Dimension "Research," signifying the highest correlation within the dataset. This analysis affirms that the Dimensions and Indicators chosen by "U-Multirank" form a robust set for assessing Academic Institutions, with minimal instances of redundancy in the measurements.
The current study delves into the academic international multidimensional ranking known as "U-Multirank," comprising five Dimensions and 36 Indicators. Capes explored this ranking while preparing the new evaluation of Post-Graduate programs in Brazil (CAPES, 2019), warranting further investigation utilizing existing data to anticipate the potential outcomes when applied within the Brazilian context.
The "U-Multirank" compiles a list of "best performers" by country annually, a challenging task given the multidimensional nature of rankings. Our research demonstrates various methods for creating such a classification. While "U-Multirank" adopts an "Olympic Medals Table" format, highlighting institutions with the highest number of maximum scores, our study identifies issues with this approach. We propose two alternative methods for general ranking: calculating the simple average of all indicators and averaging the dimensions. The latter, favored in our study, utilizes all available data and assigns equal weight to each dimension. This distinction is crucial, particularly as Capes may necessitate diverse classification methods when employing multidimensional evaluation techniques.
The analysis of missing data revealed that it is a significant issue within this ranking. Specifically, for the "top 300 performers" in Europe in 2020, the percentage of missing data stands at 10.20%. However, this figure escalates when extending the analysis to include institutions from other continents and those that do not perform as well.
An examination of the indicators reveals their high level of objectivity, characterized by clear definitions and reliance on numerical data governed by pre-established rules. The significance of trustworthy data sourced from institutions is evident, as demonstrated by the distribution of indicators: 22 (61.11%) are derived from questionnaires, underscoring the emphasis on first-hand information, while only 14 (38.89%) originate from open sources.
An overview of the average magnitudes of the Indicators across each Dimension validates a substantial degree of independence among the majority of the Indicators, with the highest average correlation coefficient of 0.3515 observed in the Dimension labeled "Research" within the cohort of institutions examined in this study.
Based on the analysis of the top 300 performers in Europe in 2020, it can be asserted that the Dimensions and Indicators chosen by U-Multirank constitute a robust framework for assessing Academic Institutions, displaying minimal redundancy in measurement criteria. The multidimensional methodology adopted by U-Multirank holds significant value, as it acknowledges the inherent diversity in institutional performances across various Dimensions. This approach is crucial as most institutions exhibit disparities in their strengths and weaknesses across different Dimensions, rendering traditional classifications inadequate in capturing these nuanced variations.