We come to this book on “predatory practices in scholarly communication” as members of a 
project that develops journal publishing platforms and conducts research on open science. In this 
chapter, we work with a set of 521 journals using that platform that also occupy a place on one or both 
of the two significant lists of journals said to be “predatory.” One of the lists, representing 30,968 
journals from “potential, possible, or probable predatory scholarly open-access publishers,” was 
maintained until 2017 by University of Colorado Denver librarian Jeffrey Beall (Beall’s list, n.d.). The 
other, which has assembled 15,470 titles since 2017 in a “database of journals [which] our specialists 
have flagged as probable threats,” is Cabells Predatory Reports (Cabells, n.d.). The publishers and 
journals on these lists are presumed to prey on researchers, luring them to pay an “article processing 
charge” (APC) to publish in what is only the pretense of an open access scholarly journal.1  
We write “presumed to prey” because of how difficult it is for Beall, Cabells, or any other 
observer to know whether a journal is adhering to such scholarly standards as peer review. The 
challenge stems from how journals arose out of, and often continue to be, the work of scholarly 
societies and groups consisting of trusted colleagues (Csiszar, 2020). This has meant that editorial 
transparency has not been an issue, apart from a journal’s listing of well-respected names on the 
masthead. Now that the internet and open access have broadened the global scale on which an 
expanded array of research is produced and circulated, those given to deception can hide behind this 
tradition of trust. Without access to a journal’s editorial processes, Beall and Cabells rely on proxies 
for “probable threats” to scholarly integrity, such as unprofessional websites, incomplete mastheads, 
exaggerated claims, and email spamming.  

Predatory proxies, however, prove to be problematic. They frequently turn out to apply to well-
established journals, including top-tier titles (Olivarez et al., 2018).2 They are used by some in 
equating predatory with open access publishing more broadly, reflecting Beall’s own outspoken 
opposition to open access (Beall, 2013; Krawczyk & Kulczycki, 2021). On the other hand, efforts to 
directly assess journals’ adherence to the peer-review gold standard have proved questionable and 
mixed. By relying on authors’ estimation, for example, Cobey et al. (2019) found that 83.3 percent of 
those publishing in Beall-listed journals believe their work was peer-reviewed, which is less than 
reassuring. More convincingly, the journalist John Bohannon submitted a hoax paper to over 304 
journals (2013). With Beall’s list, Bohannon reports that 18 percent of the journals rejected his fatally 
flawed paper (compared to 62.4 percent rejection overall), while among the minority that accepted it 
were journals from the leading publishers, Elsevier, Wolters Kluwer, and SAGE.3 Then there are the 
researchers who appear to exploit predatory journals for the increased compensation and research 
awards from their institutions that follow from increased publication (Demir, 2018; Pyne, 2017;). 
Despite these reasons for approaching the issue with caution, the growing sense is that “predatory 
journals are a global threat,” as some 35 scholars declared in Nature (Grudniewicz et al., 2019), which 
may be unduly undermining what might otherwise be a welcomed global expansion of research.4 
The scholarly publishing industry has responded to the phenomenon with a “Think. Check. 
Submit.” campaign. The campaign website advises authors “to check if [the journal] is trusted” before 
submitting a paper (Think, n.d.). This means relying on journals that “you and your colleagues know,” 
that are indexed, and that belong to a trade organization like those sponsoring this campaign. The 
website allows that some well-intentioned journals are mislabeled “predatory” for want of resources, 
but the overall thrust is that as “more research is being published worldwide… many researchers have 
concerns about predatory publishing.” From our perspective, at least, many researchers also have 
concerns about how to facilitate a more open science through open access, open infrastructure, and 
related initiatives, which is where this chapter comes into the picture.  
Our three-phase study represents a response to the question of what scholarly publishing 
platform developers and researchers can do to address the combined problem of fake journals and 
“predatory” mislabeling, both of which are undermining scholarly publishing. The study works with 521 
journals that are both listed as “predatory” (by Beall and/or Cabells) and employ Open Journal 
Systems (OJS), a free open source editorial management and publishing platform. While Beall’s list 
remains freely available online, Cabells Predatory Reports is only available by subscription for which, 
at our request, Stanford University subscribed in 2021 ($3,500). OJS is developed by the Public 
Knowledge Project (PKP) at Simon Fraser University and Stanford University. As members of PKP 
and in the service of full disclosure, we acknowledge two conflicts of interest, as well as a sense of 
responsibility, that underlie our research into these journals.  

First of all, our findings bear directly on the reputation of PKP’s software and those who 
employ it for their journals, as well as on the reason for this open source software project, which is to 
support open access to research as a human right and provide a means of improving this body of 
work. The data from the OJS journals used in this analysis have been made available for purposes of 
reproducibility and further studies, as a check on possible bias (Khanna et al., 2021). A second 
conflict of interest is rooted in how open source software projects, such as OJS, are generally 
committed to respecting users’ “freedom to run the program for any purpose,” to cite a common open 
source software definition (Wheeler, n.d.).5 Yet rather than taking the typical open source “hands-off” 
approach to software’s users, we are prepared to intervene out of a responsibility to assess and affect 
where OJS fits into the “predatory” picture. Our goal is to better understand the role that open source 
software and open infrastructure platform developers can play in addressing this issue.  

Our intervention is two-fold: (a) We provide OJS-using publications identified as predatory with 
ways of addressing the seeming reasons for the label with the goal of improving their scholarly 
publishing quality; and (b) we are about to add verification technologies and communication strategies 
to publishing platforms by which readers will be in a better position to assess journal integrity. The 
overall goal here is to reduce the confusion and harm that this phenomenon is causing in scholarly 
communication, while raising the quality of scholarly publishing. Although providing such help, in the 
first instance, may equip bad actors with a better means of bluffing more authors and readers, we 
place this risk against writing off a substantial body of legitimate research and against new efforts to 
raise the technical bar for practicing deception in scholarly communication. Still, readers are advised 
to read this chapter with these conflicts of interest and sense of responsibility in mind. 

First released in 2002, the use of OJS has grown to 25,671 active journals in 2020, publishing 
in 155 countries, with 81.7 percent originating in the Global South, led by journals in Indonesia, Brazil, 
and the USA, and with research published in 56 languages, led by English, Indonesian, and 
Portuguese (Fig. 1).6 These journals published an average of 38.8 articles in 2020, and over 4.7 
million articles since 2010. PKP gathers this and other data from these journals through the software’s 
optional beacon feature. The beacon provides PKP with access to journal data, although a portion of 
journal users turn the beacon off, implying the numbers reported here are undercounts. Other studies 
have found that the journals using OJS are largely open access (89 percent), and account for 60 
percent of what are termed “diamond open access” journals, neither charging readers subscription 
fees nor authors APCs (Alperin et al., 2017; Becerril et al., 2021; Edgar & Willinsky, 2010;). While 
largely indexed by Google Scholar and in more limited ways by the Directory of Open Access 
Journals, the journals utilizing OJS represent an emerging force in research that includes a mix of 
century-old journals, new and inexperienced publishers, and a few outright crooks.7  

This study took place in three phases from 2018 to 2021. The initial phase involved reaching 
out to a small sample of publishers and journals using OJS that appear on Beall’s list and in Cabells 
Predatory Reports to see if they would be receptive to suggestions on improving their journals’ quality. 
The second phase sought to establish how many journals using OJS are to be found on Beall’s list 
and in Cabells Predatory Reports. The final phase represents a technical response to the first two 
phases. It proposes ways for the scholarly publishing industry to both verify and communicate to the 
public a journal’s adherence to scholarly standards, as the long-standing lack of transparency makes 
predatory practices possible while leading to the uncertainty surrounding, and likely misuse of, the 
“predatory” label.  

In this initial phase, conducted from July to December of 2018, we worked through Beall’s 
2017 list until we had identified 50 publishers and 51 standalone journals using OJS. We then emailed 
the publishers and editors and publishers, noting their appearance on Beall’s list and offering to 
provide guidance on their journal websites. Of those contacted, 14 publishers (representing 113 
journals) and two of the standalone journals responded with interest. We then reviewed example 
journals for each publisher. We were guided, in part, by Beall’s Criteria for Determining Predatory 
Open-Access Publishers, for which the 54 bullet points range from “no single individual is identified as 
any specific journal’s editor” (p. 2) to “the publisher has an optional ‘fast-track’ fee-based service for 
expedited peer review which appears to provide assured publication with little or no vetting” (2015, p. 
6). Two of the publishers, Scholar Science Journals and Khalsa Publisher upgraded several features 
within a month of our emails, while COES&RJ (Center of Excellence for Scientific & Research 
Journalism) responded that it was acting on our advice.8 Three of the fourteen publishers stated that 
they had unsuccessfully petitioned Beall to take them off the list, while a fourth convinced Beall to 
note their inclusion in the Directory of Open Access Journals (DOAJ).  

In our analysis of the 14 publishers and two standalone journals, we discovered that seven of 
the publishers (or 14 percent of the publishers randomly chosen at the outset) did not charge authors 
for publication, which basically disqualifies them from the Beall and Cabells characterization of  
“predatory.” Two of the seven sold subscriptions to their journals, and five had neither subscriptions 
nor author charges. In addition, we checked the entire set for compliance with what we judged to be 
eight key criteria from among Beall’s set (2015), to which we added DOAJ and Google Scholar listings 
(Table 1). For those that charged authors, only one publisher (Fundamental Journals) did not comply 
with seven or more of the 10 scholarly standards.  

On the peer-review question, we asked the 14 publishers who responded to our original 
inquiry, if they would allow us limited use of password access to their peer review process (based on 
our knowledge of OJS). Five publishers granted us access to a journal (Table 1). In spot-checking an 
average of 12 recent articles per journal, we found four of the five publishers’ journals had complete 
sets of reviews, while the fifth was missing reviews for three articles out of 20. There were 1.6 reviews 
per article on average, although 24 percent of the reviews contained only a recommendation to 
publish without comment, suggesting that there is much work to be done on improving peer review 
quality. In sum, of the original randomly selected 50 publishers using OJS on Beall’s list, 14.0 percent 
largely compliant with Beall’s basic criteria, with five of the seven providing direct evidence of peer 
review. 

While Beall did not specify the reasons that individual publishers or journals were added to his 
list, Cabells Predatory Reports (2021) identifies the “violations” for each title, ranking them from 
“severe” to “minor.” Cabells’ “60+ behavioral indicators” for journals are described as indicative of 
“misconduct,” “fraudulent operations,” and “probable threats” (Cabells, n.d.). For the journals using 
OJS in its list, Cabells identified 4.3 violations, on average, per journal. The most common violation 
was that the journal failed to possess or post a policy for digital preservation. This “moderate” violation 
is included for 107 or 45.1 percent of the journals using OJS (Table 2). For each of the publishers that 
had one or more journals in Cabells Predatory Reports, we prepared an email for publisher and 
editors that listed their violations. After all, it would be very unlikely that they would otherwise have 
access to this list. We also included our recommendations for addressing these concerns and the 
method of notifying Cabells of the corrections that they may have made to their journals. For the most 
common violation, for example, we recommended that the publisher sign up their journals for the PKP 
Preservation Network, available to all journals using OJS and post this as their preservation policy. As 
the emailing of the publishers was only completed at the point at which we submitted this chapter for 
publication, we are not in a position to report on its success in helping the publishers.    

Cabells method of identifying “violations” for each title listed is a step up from Beall’s 
method of simply adding publishers and journals to its list. Yet there remains a reliance on relatively 
weak proxies for what may or may not reflect a lack of experience and professional support. As well, 
the trade-off for this gain in detail is that the journals’ identities, as well as their “violations,” are 
unlikely to be available to those listed. This is of concern because Cabells’ list of violations does 
helpfully identify ways to improve the journals. We have suggested to Cabells the basic fairness of 
sharing its assessments with the journals designated a “probable threat.” While Cabells is not 
prepared to undertake such a step at this point, we plan to update the company on the results of our 
strategy of reaching out to the publishers with journals in Predatory Reports with advice for 
addressing the “violations” attributed to their publications and for seeking reconsideration. Our hope is 
that there may yet be a reason for this company to reconsider its contributions to scholarly publishing. 
This approach might also lead to an increased accuracy of their predatory reports by excluding false 
positives. 

Nonetheless, our experiment of reaching out to publishers and journals has had limited 
success. What we found adds to the literature on predatory list overreach. However, a response rate 
of 28.0 percent among publishers and 3.9 percent among standalone journals, with only two 
publishers acting on our suggestions, suggests that while this may be the right thing to do, it is not an 
effective strategy for rectifying this issue, which calls for increasing certainties around identifying 
fraudulent journals.  

In this phase, we set out to determine the extent to which journals using OJS are identified as 
“predatory” in Beall’s and Cabells’ lists. In the first instance, we compared the PKP list of journals 
using OJS with Beall’s final list of predatory publishers and journals, which he suspended in 2017 
after several publishers and organizations fought back against such listings (Silver, 2017). To 
establish how many journals Beall’s list ultimately represented in 2018, we counted the journals in a 
sample of 231 publishers (19.9 percent of the 1,163 publishers) without regard to the use of OJS 
(Table 2). The average we found of 24.3 journals per publisher suggested that Beall’s list represents 
30,968 journals as “predatory,” including the 1,395 standalone journals. We also found that 61.7 
percent of these journals had not yet published an article, while 6.3 percent did not have a website, a 
proportion that rose to 32 percent by 2021.9  Of journals using OJS, 366 titles are associated with 
Beall’s list, amounting to 1.2 percent of the journal total for Beall’s List and 1.4 percent of the 25,671 
active journals known to be using OJS (Table 3).10 

For its part, Cabells International was generous enough to undertake a comparison of PKP’s 
list of journals with those in Predatory Reports by matching the journal’s ISSNs (International 
Standard Serial Number) across the two lists. Limiting the match to journals with ISSNs reduced 
Cabells’ list to 7,490 titles (out of 15,470) and the PKP’s to 22,802 (out of 25,671). Within this set, 237 
journals appeared on both lists, representing 3.2 percent of Cabells list (with ISSNs), and 1.0 percent 
of the journals using OJS (with ISSNs).  

We examined the overlap among journals using OJS that appear on both the Beall’s and 
Cabell’s lists (Table 4). We found that 82 journals (0.3 percent of the OJS total) appear on both lists, 
led by journals published in India and the United States.11 Taken together, a total of 521 journals 
using OJS appear on one or both predatory lists, amounting to 2.0 percent of the journals known to be 
using OJS. Between the two lists for those journals using OJS, Cabells appears to have a somewhat 
greater focus on the Global South, while the country differences between the two lists are likely the 
result of publishers’ journal sets. 

We take little comfort from the proportion of journals using OJS on these two lists. An open 
source publishing platform that is free to download and documents setting up and operating journals 
might have been expected to be more widely used by fake journals. It may be that OJS’ design, 
dedicated to providing editorial oversight of peer review, is off-putting to those with no such intent, or 
that the platform’s design and support enable journals to rise above the subjective judgments behind 
the “predatory” label. Still, some journals are almost certainly using OJS to illegitimately charge 
authors for publishing their submissions without peer review to the detriment of science. While there is 
evidence of overcounting on both lists, the proportion of journals using OJS is higher in the more 
recent list maintained by Cabells Predatory Reports (3.2 percent) than in Beall’s list (1.2 percent), just 
as Cabells appears to be providing greater coverage of journals in the Global South than Beall. This 
increase may reflect OJS’ growth rate, which only adds to our responsibilities as platform developers 
to address this issue.      

Also troubling is the scale of the uncertainty and innuendo to Beall’s list and Cabells Predatory 
Reports. Yet rather than blame Beall and Cabells International, it may be time to redirect the scholarly 
publishing technologies that have made this global knowledge exchange possible. New systems are 
needed that can verify and demonstrate to the public the extent to which these journals adhere to 
scholarly standards, which bring us to the third phase of this study. 

To support the public value of open access and the global expansion of scholarly publishing, 
PKP is developing new tools for assessing and communicating scholarly trustworthiness. This will 
involve journals turning to third-party trade organizations to verify and register who is doing what in 
the publishing process, with an example involving ORCiD, a researcher identity and profile 
management organization, presented below. Initially, the goal is to work with five basic scholarly 
standards, before considering more granular and specialized standards around, for example, clinical 
trials (Table 5). These systems will depend on the exchange of information between journals and 
these organizations, with an openness to a level of scrutiny not possible today, whether in seeing the 
background of a journal’s reviewers or how many reviews a paper is typically subject to. While 
involving automated connections and controls, the outcomes will be subject to human review and 
challenge. Through open source licensing of the connecting technologies, such developments will 
also be made available to commercial and other platforms.  

For example, when editors, editorial board members, reviewers, and authors initially register 
with a journal’s publishing platform, they will be required to log into ORCiD (Fig. 2). To be listed as the 
editor of a journal would involve a further log in with ORCiD in which this new position would be added 
to one’s ORCiD profile, while ORCiD will provide the journal with a hyperlinked ORCiD icon, enabling 
readers and authors to explore the editor’s background and qualifications knowing that the identities 
have been authenticated and that appeals can be made to ORCiD if anything seems amiss. Such 
systems may be susceptible to circumvention, as no technology is foolproof, of course, but the effort 
required to do so without detection will have been greatly increased and in ways that can be further 
improved in the face of violations.  

Then there is the question of how the results of this and other verification systems, whether for 
peer reviews, indexing, and other elements, will be communicated to readers and authors. Here we 
have begun to develop a “Publication Facts” label, based on the common FDA Nutrition Facts label 
(Fig. 3). This approach will be not only be open to public scrutiny, following open science principles, 
but also assessed and refined with various audiences, from high school students to journalists, to 
ensure the label’s clarity and comprehensibility with researchers and members of the public who 
should be able to use the label to assess the trustworthiness of research articles.12 The label, which 
will be linked to individual studies, will provide metrics on their compliance with standards, along with 
detailed explanations of each standard and metric. Such labelling is intended to inform and educate 
the public and the professions on research standards, while providing a basis for readers to briefly 
consider or explore in more detail the trustworthiness of research publications. 

Although the lack of transparency and clarity in the degree to which journals adhere to 
scholarly standards applies to the larger world of scholarly publishing, these verification and 
communication systems will also, of course, help reduce the number of journals mislabeled as 
“predatory.” To make journals adherence to scholarly standards explicit in publicly accessible ways 
may well encourage wider use of this work. Journalists and other professionals would get into the 
habit of checking the label before using research, while reading such labels could well form part of 
what high school and college students would learn about science. Such an industry standard for 
scholarly publishing seems to go hand in hand with universal open access and public support for 
research.  

No one doubts that unscrupulous website operators are present in scholarly publishing, much as they 
are in other fields. Without denying this reality, this study joins others in demonstrating how the lack of 
transparency at important points in the scholarly publishing process can lead to an over-application of 
the predatory label to journals and publishers. While this may increase awareness of a real problem, it 
threatens the progress that open access is making in the emergence of a greater global research 
effort. Nowhere is this more apparent than the industry’s current response with “Think. Check. 
Submit.” While it is aimed at assisting authors considering where best to submit their work, it cannot 
help but foster a broader distrust of the research literature beyond familiar and recognized 
publications.  

What this study adds to the considerable literature on predatory journals is both evidence and 
reason for addressing the underlying issues of transparency. By developing verification systems for 
publishing platforms involving trusted trade organizations, the bar is raised for both those operating 
predatory journals and those (mis)applying the label. While PKP is taking the lead with these systems, 
we recognize that their effectiveness will depend on their adoption as an industry standard for journal 
accountability across publishers and publishing platforms. This will involve a wide range of journal 
platforms and scholarly publishers that share a common goal of assisting the public in assessing the 
trustworthiness of research publications, given their growing open access to research. These 
standards for verification and authentication, especially as they are attuned to communicating to the 
public, as well as professionals, the publishing practices that distinguish scholarly publishing, will raise 
the bar for both legitimate and deceptive journals. 

If we can provide a publicly accessible, trustworthy basis for having greater confidence in a 
journal’s legitimacy, then services such as Cabells might be willing to shift their efforts from 
assembling lists of potential offenders to more directly protecting the public interest by working with 
those journals in need of corrections and other improvements, while still seeking to expose deliberate 
acts of deception and fraudulence. What Jeffrey Beall and Cabells International have exposed, above 
all, is the need for means of verifying and communicating journal adherence to scholarly standards in 
an age of open access and global participation in research.  

Gary Schwartz is to be thanked for undertaking the data collection in phase one of this study; as are 
Juan Pablo Alperin, Jon Ball, Jonas Raoni, and Alec Smecher for the OJS beacon data set and 
Miroslav Suzara for the Cabells analysis used in phase two; Juan Pablo Alperin (also for editing), 
Lauren Maggio, and Laura Moorhead for contributions to phase three; Kyla Chasalow for thoughtful 
comments throughout; Doug Dworkin for the graphic design of Fig. 2 and Fig. 3; Kathy Kerns, Shani 
Braier Marcovitz, Matt Marostica, Sarah Forsetting, Jennifer Uchiyama and Greta deGroat for 
obtaining Stanford University Libraries access to Cabells Predatory Reports; and Simon Linacre of 
Cabells International for assistance in identifying journals using OJS in Cabells Predatory Reports. 
