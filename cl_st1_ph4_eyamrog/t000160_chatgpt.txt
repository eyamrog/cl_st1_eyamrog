This study aimed to assess the content validity and reliability of the Questionnaire for Screen Time of Adolescents (QueST). The QueST evaluates screen time across five categories: studying, working/internship-related activities, watching videos, playing video games, and using social media/chat applications. Content validity, involving a pretest, was conducted by experts and adolescents. To evaluate reliability, QueST was administered and readministered after one week to a cohort of 104 adolescents (mean age 16.3 ± 1.02 years; 66.3% female). Findings revealed a Content Validity Index for Scales of 94% and 98% for overall clarity and representativeness, respectively. Adolescents found QueST to be easily understandable and clear. Intraclass correlation coefficients ranged from 0.41 (95% CI 0.24, 0.56) for videos to 0.76 (95% CI 0.66, 0.83) for social media/chat applications on weekdays, and from 0.24 (95% CI 0.04; 0.41) for videos to 0.67 (95% CI 0.54; 0.77) for social media/chat applications on weekends. In conclusion, QueST exhibited satisfactory content validity, although assessing video watching time during daily activities posed a challenge. Overall, QueST is recommended for measuring various screen time behaviors.
Screen time behaviors refer to interactions with electronic devices such as watching television or using smartphones, which can occur in recreational, professional, and educational contexts (Tremblay et al., 2017). Screen time, often involving activities like television viewing, computer use, or video game playing, has been linked to negative outcomes in children and adolescents (Biddle et al., 2017; Biswas et al., 2015; Carson et al., 2016; de Rezende et al., 2014; Tremblay et al., 2011). Traditionally, activities like watching movies and playing video games were confined to specific devices like televisions and consoles, but technological advancements have made them accessible on various gadgets such as computers, tablets, and smartphones. These changes have led to shifts in screen time behaviors, with a decrease in television usage and an increase in computer use among adolescents (Bucksch et al., 2016; Silva et al., 2014). The evolving landscape of activities on electronic screens also impacts health outcomes. For instance, the World Health Organization now recognizes video game addiction in the International Classification of Diseases-11, highlighting its detrimental effects on health (World Health Organization, 2018). Similarly, excessive social media use, a relatively recent phenomenon, has been associated with depressive symptoms (da Costa et al., 2020), socialization issues (Arundell et al., 2019; Devine & Lloyd, 2012; Ihm, 2018), poor body image (de Vries et al., 2016), and lower academic performance (Kuss & Griffiths, 2011).
The multitude of activities that can be carried out on electronic devices, such as playing games, watching videos, and accessing social media, presents a new set of challenges in quantifying screen time. Recent reviews on questionnaires designed to measure sedentary behavior have revealed that most tools typically feature a single question, often focusing on time spent watching TV, playing video games, or using computers (Hidding et al., 2017; Prince et al., 2017). However, given that the impact of each of these activities on health outcomes may vary (Biddle et al., 2017; Carson et al., 2016; da Costa et al., 2020; Ihm, 2018; Weaver et al., 2010), it remains crucial to differentiate between these activities to enhance our comprehension of the origins of health issues in young populations. Consequently, this study seeks to introduce a questionnaire that can assess various aspects of screen time among adolescents and subsequently evaluate its content validity and reliability.
The Questionnaire for Screen Time of Adolescents (QueST) was developed to assess habitual screen time volumes across various constructs in the adolescent population. Following its creation, the QueST underwent a three-step psychometric evaluation process, each step involving a distinct sample: i) content validity was assessed with input from 16 experts in the field of adolescent screen time research; ii) pretesting involved 14 adolescents from a Federal Institute of Technological Education in Santa Catarina state; iii) reliability was evaluated using a sample of 104 high school students from the Aplicação school in Santa Catarina state. These evaluations were carried out in 2019, with all participants and their parents/legal guardians consenting to the study protocols and providing written consent forms. Approval for this study was granted by the ethics committee for research involving human participants at the Federal University of Santa Catarina, Brazil (protocol number: 3.168.745).
The QueST was designed to assess screen time on both weekdays and weekends across various dimensions. The instrument's initial construction adhered to established guidelines (Hidding et al., 2017) and commenced following a cursory review of recent literature on questionnaires for sedentary behavior assessment (Hidding et al., 2017; Prince et al., 2017). The development process of the QueST involved: i) identifying the key constructs; ii) determining the questionnaire's administration format; iii) selecting the number, format, order, and wording of items and response choices; iv) reviewing the questionnaire for optimization of organization and readability (de Vet et al., 2011; Tsang et al., 2017).
Five constructs related to screen time were delineated based on inquiries utilized in studies concerning sedentary behavior (Cerin et al., 2014; Guimarães et al., 2013; Hidding et al., 2017; Prince et al., 2017; Treuth et al., 2003). These constructs encompassed: (i) activities associated with academic study or homework; (ii) tasks linked to employment (encompassing internships and volunteer work); (iii) viewing videos, including series, movies, news, and sports; (iv) engaging in video gaming; and (v) utilizing social media and chat applications. The decision to amalgamate the assessment of chat applications and social media into a single construct was motivated by the fact that these platforms typically offer both services (e.g., direct messaging capabilities on Facebook, Instagram, and Twitter). The inclusion of the work-related construct was justified by the screen time demands of certain internships and occupations. For each construct, the duration in hours and minutes could be documented for weekdays and weekends.
The QueST was originally developed in Brazilian Portuguese and intended for self-administration by adolescents using a smartphone, tablet, or computer with internet access. The instrument was hosted on the SurveyMonkey® platform. Each construct described in the questionnaire corresponded to an individual item. All items included specific instructions: "Enter zero if you do not participate in these activities," followed by an illustrative example (e.g., "Example: I watch series for 1 and a half hours per day [enter 1 in the hours field and 30 in the minutes field]"). The specific QueST items can be found in Table 1.
For content validation, a panel of experts was carefully chosen from individuals actively engaged in research projects, monographic works, and publications in reputable scientific journals focusing on screen time behaviors or studies involving psychometrics and questionnaire validation. Each expert held a doctoral degree and held positions as professors or researchers in universities or research institutions. These experts were approached via email, receiving an invitation letter that introduced the QueST tool and elucidated the reasons behind its creation. The invitation included a comprehensive background document outlining key concepts and providing a detailed explanation of how the tool could be applied and utilized in research endeavors.
The QueST was evaluated by experts in two distinct steps: firstly, through an individual assessment of each item, and secondly, through a comprehensive evaluation of the questionnaire as a whole (Polit & Beck, 2006). The experts independently assessed the content validity of the questionnaire, scrutinizing each item for clarity and representativeness (Rubio et al., 2003). The clarity assessment focused on the wording of the questions in relation to the understanding of the construct being measured (Grant & Davis, 1997), while the representativeness evaluation aimed to ensure that the items accurately reflected screen time, its constructs, and associated concepts (Grant & Davis, 1997). Experts meticulously analyzed each item and the response scale, providing ratings on clarity using a 4-point Likert scale (4 = highly clear; 3 = quite clear; 2 = somewhat clear; 1 = not clear), and on representativeness using a similar scale (4 = the item is representative; 3 = the item needs minor revisions to be representative; 2 = the item needs major revisions to be representative; 1 = the item is not representative) (Rubio et al., 2003). Following the assessments, the Content Validity Index for each question was calculated based on the ratings for clarity and representativeness (Polit & Beck, 2006). Additionally, experts could provide general comments on the questions to further enhance the evaluation process.
In the global assessment of the QueST, experts were asked to evaluate the clarity and expressiveness of the title (yes/no), the representation of adolescents' screen time in all items (yes/partially/no), the appropriateness of the metric (yes/partially/no), the suitability of the unit of measure and response scale (yes/partially/no), the adequacy of the item sequence (yes/partially/no), and the use of bold tags to highlight key information in the online questionnaire (yes/partially/no). Experts were also encouraged to provide feedback on each item, including suggestions for adding or removing items.
This step aimed to assess the comprehension of the questions and response scales proposed (Borsa et al., 2012), as well as to identify any ambiguity, misinterpretation of items, and potential difficulties (Presser et al., 2004) within the QueST. To achieve this, a convenience sample of 14 high school students from a Federal Institute of Technological Education in Santa Catarina state participated in the review process. The students were presented with an online questionnaire that included the QueST and supplementary questions regarding: (i) the clarity of each item (highly clear/quite clear/somewhat clear/not clear); (ii) identification of unfamiliar words in the items (no/yes, specify); (iii) understanding of how to answer the QueST (I did/I did not understand); (iv) any difficulties encountered while answering the QueST (no/yes, specify); and (v) any perceived lack of other activities involving electronic screens in the questionnaire (no/yes, specify). This assessment took place in a classroom setting during school hours, with students accessing the questionnaire via a provided electronic link on their smartphones.
In order to assess the reliability of the QueST, all high school students from the Aplicação school were enlisted, and those who consented to take part were requested to complete the QueST on two occasions with a seven-day gap between administrations (de Souza et al., 2017). This process was carried out in a classroom setting during regular school hours, with students accessing the questionnaire via an electronic link on their smartphones. The testing conditions remained consistent for both the initial test and the retest, including the administrators present, the environment, and the instructions provided.
The clarity and representativeness of the five items in the QueST were assessed using the Content Validity Index for Items (I-CVI) as outlined by Polit and Beck (2006). The I-CVI scores were determined by summing the ratings of either “3” or “4” for each item and dividing by the total number of experts. Additionally, the Content Validity Index for Scales (S-CVI) was calculated as the average of the I-CVIs, separately computed for clarity and representativeness. Qualitative feedback from experts was analyzed by authors MTGK, BGGC, and PCS, with suggestions either accepted or rejected based on consensus after thorough revision and discussions among the three authors. This process was conducted in a blinded manner to maintain expert anonymity and minimize potential biases.
The evaluation of the QueST by students was presented descriptively through proportions. Any ratings of "somewhat clear" or "not clear" regarding the wording of an item, as well as responses indicating a lack of understanding on how to answer the QueST, served as criteria for reformulating either the specific item or the entire instrument, leading to a subsequent evaluation by the students. Additionally, authors MTGK, BGGC, and PCS collectively agreed to substitute unfamiliar words with simpler alternatives. Suggestions for additional activities involving screen media devices were considered for inclusion in the questionnaire. The challenges encountered by students in responding to the QueST were also detailed.
Only students who completed both the test and retest measures were included in the reliability analysis, with those having missing data or providing implausible answers (defined as >14 daily hours) being excluded. To assess stability, differences between the test and retest scores were examined using Student's t-tests. Given the skewness of some variables, additional non-parametric tests (Sign-Rank tests) were employed to validate the results. The consistency of the constructs was evaluated using intraclass correlation coefficients (ICC), while Bland-Altman dispersion analyses were utilized to investigate the discrepancies and limits of agreement (in minutes) between the test and retest measurements.
Out of the 24 experts invited, 16 (66.7%) responded to the questionnaire, while eight experts chose not to provide answers without specifying a reason. Table 2 displays the I-CVI and S-CVI values assessing the clarity and representativeness of the QueST. In terms of clarity, the lowest I-CVI score was noted in Item 1 (studying) at 0.88, indicating an 88% agreement among the experts. Items 2, 4, and 5 achieved an I-CVI of 0.94, while Item 3 (watching videos) garnered unanimous agreement at 100%. The overall clarity of the QueST, as indicated by the calculated S-CVI, stood at 94%. Moving on to representativeness, four out of the five items were deemed 100% representative, with playing video games (item 4) scoring an I-CVI of 0.88. The S-CVI value for representativeness was calculated at 98%.
Following expert review, modifications were made to the questionnaire title, with certain terms in the items being either replaced or added (e.g., "sports" was included in the third item). The response scale underwent simplification, as suggested by the experts, resulting in a shorter scale with 10-minute intervals (0, 10, 20 minutes...) rather than a minute-by-minute scale. Additionally, experts played a role in reorganizing the items to minimize cognitive load, although no items were added or removed in this process.
In the initial evaluation of the QueST, fourteen students (aged 18.2±1.0 years, 42.9% female) participated. All participants found the question wording to be either highly clear or quite clear across the items (Item 1: 71.4% highly clear, 28.6% quite clear; Item 2: 78.6% highly clear, 21.4% quite clear; Item 3: 85.7% highly clear, 14.3% quite clear; Item 4: 84.6% highly clear, 15.4% quite clear; Item 5: 84.6% highly clear, 15.4% quite clear), with no ratings of "somewhat clear" or "not clear." None of the students reported vocabulary issues, and all comprehended how to respond to the QueST. While the majority (78.6%) had no trouble answering the questions, three students mentioned difficulty in accurately reporting their habitual screen time. Despite this, based on student feedback, no adjustments to the QueST were deemed necessary.
Out of the 203 eligible students, 104 students willingly agreed to partake in the study, submitting written informed consent forms and completing the QueST in both the initial test and the subsequent retest. The participants had an average age of 16.3 years, with a majority being girls (66.3%). Analysis revealed that the average duration of social media usage on weekdays was greater during the test phase, while studying on weekends showed an increase during the retest phase. Interestingly, the time spent watching videos on weekend days was found to be higher during the test phase in comparison to the retest phase, as indicated in Table 3.
All intraclass correlation (ICC) values demonstrated statistical significance as outlined in Table 4. The most substantial ICC was identified in the utilization of social media during weekdays (ICC= 0.76, 95% CI 0.66; 0.83), while the least pronounced ICC was noted in the category of watching videos over weekends (ICC= 0.24, 95% CI 0.04; 0.41).
The Bland-Altman analyses for the QueST constructs are detailed in Table 5. The mean differences varied across activities, with a range from -4.6 minutes (Upper limit: 149.6; Lower limit: -158.7) for weekday work to 40.6 minutes (Upper limit: 400.0; Lower limit: -318.9) for weekend video consumption.
The QueST demonstrated sufficient capability in evaluating various screen time constructs with acceptable content validity. Nevertheless, the stability of the items exhibited significant variation across different constructs and days examined (weekday versus weekend). The content validity was deemed appropriate based on the consensus among experts regarding the clarity and representativeness of the items and the instrument. Adhering to the acceptability criteria for items, which includes the standard error of the proportion of agreement, the lowest I-CVI accepted is 0.78 within a panel of experts consisting of 6 or more individuals (Lynn, 1986; Polit & Beck, 2006). Furthermore, the instrument as a whole demonstrates acceptable content validity when the S-CVI is ≥0.90 (Waltz et al., 2005). Moreover, the feedback provided by experts proved to be valuable in refining the instrument through textual modifications and minor additions, obviating the need for another round of review by the expert panel. In conclusion, the QueST emerges as a reliable tool for assessing screen time among adolescent populations.
The evaluation of the instrument based on the target population and expert input is strongly advocated (Hidding et al., 2017; Mokkink et al., 2010). However, this crucial step was omitted in over 80% of studies that assessed the measurement properties of sedentary behavior instruments (Hidding et al., 2017). In an initial assessment by adolescents, the QueST was deemed clear and understandable. Nevertheless, three students encountered challenges in accurately reporting their typical screen time in each item. This issue is a common hurdle in capturing precise recollections in questionnaires aimed at measuring behaviors among children and adolescents (Kohl et al., 2000). Screen time tends to fluctuate and be influenced by various factors over time (Cabanas-Sánchez et al., 2018), potentially leading to inaccurate estimations of habitual behaviors. To enhance this estimation process, the response scale was revised to simplify comprehension for adolescents, aiding them in reporting their behaviors more effectively.
The stability of the items varied from poor to excellent (Rosner, 2005), with the sample size of this procedure deemed adequate (Terwee et al., 2007). Notably, the item concerning watching videos (including series, movies, soap operas, news, sports, programs, among others) on both weekdays and weekends exhibited the lowest ICCs in comparison to the other items. This observation could be elucidated by the fluctuating nature of this behavior between measurement repetitions, influenced by various factors that can impact screen time even within a short timeframe (Hardy et al., 2007). For instance, the release of a new season in a popular series or the occurrence of significant sporting events like the Olympic games or international league finals can substantially elevate electronic screen usage within a few days, potentially skewing either the test or retest measure. Consequently, while adolescents may accurately report their behaviors during the test and retest, the overall stability of the measurements remains poor due to the lack of consistent "typical" or "normal day" patterns in this specific behavior (Hardy et al., 2007). Future research endeavors should delve into comprehending the dynamics of video watching among adolescents over periods shorter and longer than a week to ascertain the most suitable test-retest interval for capturing population parameters.
The video game playing item exhibited fair and good reliability on both weekdays and weekends, indicating a high level of accuracy and consistency in responses to this behavior. The Intraclass Correlation Coefficients (ICCs) obtained were comparable to those reported in the Health Behavior in School-aged Children study (2008), with ICC values of 0.54 (95% CI 0.38; 0.67) on weekdays and 0.69 (95% CI 0.57; 0.78) on weekends for the gaming item (Liu et al., 2010).
The analysis of social media and chatting applications revealed consistent reliability on both weekdays and weekends, with a notable prevalence of high usage volumes. This pattern is unsurprising given the prevalent use of smartphones for extended periods throughout the day, often in conjunction with multitasking activities such as watching television. Noteworthy references (de Vries et al., 2016; Devine & Lloyd, 2012; Ihm, 2018) support this observation. Interestingly, the time spent on these platforms during the initial test measurement exceeded that of the retest on weekdays, indicating a potential fluctuation in usage patterns. While the exact reasons for this variance remain unclear, factors such as academic obligations and parental oversight could play a role in influencing these behaviors, consequently affecting the reliability of test-retest measures within a short timeframe.
The measure concerning screen time for studying during weekdays exhibited moderate to good reliability, surpassing the ICC recorded for weekends. This disparity could be attributed to the potentially fluctuating and adaptable nature of study hours during weekends, likely influenced by academic obligations like impending school exams. In contrast, weekdays may offer a more structured routine for adolescents, leading to a more consistent allocation of time towards studying.
Low volumes of screen time for both weekdays and weekends were reported, showing fair stability. A previous survey conducted in Santa Catarina, Brazil, focusing on lifestyle indicators of high school students aged 15-19, revealed that 50.5% of adolescents held jobs in 2011 (Silva et al., 2013). Notably, screen time items pertaining to study and work constructs are not frequently addressed in sedentary behavior research involving adolescents, as these aspects are often non-discretionary. Consequently, only a limited number of studies incorporate questions that access this specific information (Hidding et al., 2017; Prince et al., 2017).
The findings of this study indicate that adolescents' screen time behaviors exhibit less stability during weekends in comparison to weekdays. This variability is likely attributed to the natural fluctuations in these behaviors, particularly on weekends, rather than a decrease in the reliability of the measurement items. Factors such as the availability of opportunities for engaging in physical activities (as suggested by Hardy et al., 2007), weather conditions, and events that encourage the use of electronic devices (such as the launch of new series or games, or school exams) can influence adolescents' screen time habits on weekends. Additionally, weekends are characterized by a more spontaneous and less structured routine, allowing adolescents more freedom to use electronic devices at their discretion.
The items of the QueST utilized a "typical day" as the reference time frame to eliminate atypical events that could potentially skew measurements, like crucial exams at school that might directly impact studying. Despite this approach, the study was unable to control for all possible atypical occurrences. Additionally, adolescents' screen time habits, such as video watching, can exhibit significant variability both within and among individuals, further complicating the accuracy of responses. It is acknowledged that some bias may be inherent when behaviors are self-reported within this particular population (Kohl et al., 2000).
This study demonstrates several strengths. Firstly, it utilizes a diverse array of screen time constructs that effectively capture a significant portion of adolescents' sedentary behavior. Secondly, the study employs standardized and recommended methods for the development and validation of questionnaires, a practice not commonly observed in existing sedentary behavior instruments (Hidding et al., 2017). Thirdly, the content validity assessment focuses on the clarity and representativeness of items through a combination of qualitative and quantitative approaches. Additionally, the study incorporates the complete QueST content validation process, involving both evaluation by the target population and assessment by field experts. Lastly, the methodological procedures adhere to the Consensus-Based Standards for the Selection of Health Measurement Instruments (COSMIN) (Mokkink et al., 2010), as detailed in the Supplementary Material: Application of the COSMIN checklist on the QueST.
This study was limited by a small sample size obtained through convenience sampling in the initial test (n=14). The absence of criterion validity was noted due to the lack of a gold standard measure in free-living conditions that could serve as a reference for the five screen time constructs present in QueST. This poses a challenge for research in this area, given that screen time behaviors can vary across different devices (e.g., television, computer, tablet, smartphone). While QueST was designed to encompass activities adolescents engage in using any electronic screen device within five established constructs, not all activities could be neatly categorized within the questionnaire, such as reading eBooks for leisure.
The definitive electronic version of the QueST can be accessed at pt.surveymonkey.com/r/QLQTQHG for Brazilian Portuguese and pt.surveymonkey.com/r/Q7QXYL2 for English.
The QueST demonstrated satisfactory content validity as determined by a panel of 16 experts and received adequate evaluation from adolescents. The observed wide variability in reliability among the five items of the instrument underscores the natural fluctuation in adolescent behavior within specific screen time constructs. Overall, the QueST can be deemed a suitable tool for assessing adolescents' screen time across the five constructs outlined.
The authors express their gratitude to the Federal Institution of Education, Science and Technology of Santa Catarina and the Federal University of Santa Catarina. Additionally, they extend their thanks to all members of the expert group, which includes Filipe da Costa, Rômulo Fernandes, Evelyn Ribeiro, Adriano Hino, Grégore Mielke, Marcelo Romanzini, Leandro Rezende, Adriano Borgatto, Jeffer Sasaki, Paulo Guerra, Cassiano Rech, Leandro Garcia, Diego Christofaro, Valter Barbosa Filho, Anelise Gaya, and Andreia Pelegrini, for their valuable contributions to this research instrument.